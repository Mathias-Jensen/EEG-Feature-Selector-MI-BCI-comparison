{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383f5699",
   "metadata": {},
   "source": [
    "![image info](BCI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b4ac3",
   "metadata": {},
   "source": [
    "# Classification of brain states (FOOT and HAND Movement) framework\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Data\n",
    "i. The framework uses the dataset of healthy control subjects performing imagery right and left hand movement.\n",
    "\n",
    "ii. The total number of signals corresponds to RIGHT HAND MOVEMENT is 500000 while for RIGHT FOOT MOVEMENT it was 400000. \n",
    "\n",
    "iii. Each class has 118 channels.\n",
    "\n",
    "iv. Initial sampling rate = 1KHz, i.e. 1000 samples are captured in one second.\n",
    "\n",
    "v. The data is downsampled to 100 Hz,  i.e. 100 samples in one second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b06a9b",
   "metadata": {},
   "source": [
    "## Import required libraries (Note import is repeated to understand the required libraries are imported when specific module is implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1579a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries required to execution\n",
    "import numpy as np # linear algebra\n",
    "print(np.version.version)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for statistical data visualization\n",
    "%matplotlib inline\n",
    "import scipy.io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignoring warnings if any just to avoid confusion during the execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb51da7",
   "metadata": {},
   "source": [
    "### Load the data stored in .MAT file and assign respective classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efcdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data which is stored in MAT file\n",
    "BCI_data = scipy.io.loadmat('bci.mat')\n",
    "RH = BCI_data['rh'] # Signals belonging to right hand (RH) movement\n",
    "RF = BCI_data['rf'] # Signals belonging to right foot (RF) movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaN values in matrix2\n",
    "nan_mask = np.isnan(RF)  # Boolean mask where NaNs are present\n",
    "\n",
    "# Get row and column indices of NaNs\n",
    "nan_positions = np.argwhere(nan_mask)  # Returns list of (row, col) positions\n",
    "\n",
    "# Find which columns contain NaNs\n",
    "nan_columns = np.where(np.any(nan_mask, axis=0))[0]  # Columns with NaN values\n",
    "\n",
    "# Print results\n",
    "# Print results\n",
    "print(f\"NaN values found in {len(nan_positions)} locations.\")\n",
    "print(f\"Columns with NaN values: {nan_columns.tolist()}\")\n",
    "print(f\"First 10 NaN positions (row, col): {nan_positions[:10].tolist()}\")  # Show first 10 for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute NaN values with column mean (for median imputation just replace nanmedian)\n",
    "def impute_with_mean(matrix):\n",
    "    # Find NaN values\n",
    "    nan_mask = np.isnan(matrix)\n",
    "    \n",
    "    # Calculate column means (ignoring NaNs)\n",
    "    col_means = np.nanmedian(matrix, axis=0)\n",
    "    \n",
    "    # Replace NaNs with corresponding column mean\n",
    "    matrix[nan_mask] = np.take(col_means, np.where(nan_mask)[1])\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Impute NaN values in both matrix1 and matrix2\n",
    "RH = impute_with_mean(RH)\n",
    "RF = impute_with_mean(RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97048e9",
   "metadata": {},
   "source": [
    "### Verify if there are nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1754e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaN values in matrix2\n",
    "nan_mask = np.isnan(RF)  # Boolean mask where NaNs are present\n",
    "\n",
    "# Get row and column indices of NaNs\n",
    "nan_positions = np.argwhere(nan_mask)  # Returns list of (row, col) positions\n",
    "\n",
    "# Find which columns contain NaNs\n",
    "nan_columns = np.where(np.any(nan_mask, axis=0))[0]  # Columns with NaN values\n",
    "\n",
    "# Print results\n",
    "# Print results\n",
    "print(f\"NaN values found in {len(nan_positions)} locations.\")\n",
    "print(f\"Columns with NaN values: {nan_columns.tolist()}\")\n",
    "print(f\"First 10 NaN positions (row, col): {nan_positions[:10].tolist()}\")  # Show first 10 for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d0ae1",
   "metadata": {},
   "source": [
    "### Preprocessing using filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f15b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=7):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def notch_filter(data, notch_freq, fs, quality_factor=20):\n",
    "    nyquist = 0.5 * fs\n",
    "    notch = notch_freq / nyquist\n",
    "    b, a = iirnotch(notch, quality_factor)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "fs = 200  # sampling frequency in Hz\n",
    "# Apply notch filter to suppress 50 Hz signal\n",
    "filtered_RF = notch_filter(RF, 50, fs)\n",
    "filtered_RH = notch_filter(RH, 50, fs)\n",
    "\n",
    "# Apply bandpass filter between 0.5-30 Hz\n",
    "filtered_signal_RF = bandpass_filter(RF, 0.5, 30, fs)\n",
    "filtered_signal_RH = bandpass_filter(RH, 0.5, 30, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbde759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw and filtered signals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(RF[5000:6000,84])\n",
    "plt.title('Raw EEG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(filtered_signal_RF[5000:6000,84])\n",
    "plt.title('Filtered EEG Signal (Notch + Bandpass)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(RH[5000:6000,84])\n",
    "plt.title('Raw EEG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(filtered_signal_RH[5000:6000,84])\n",
    "plt.title('Filtered EEG Signal (Notch + Bandpass)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83a945",
   "metadata": {},
   "source": [
    "### Select any one channel for analysis and reshape the data into 10 second long epoch\n",
    "\n",
    "#### Final obtained signal\n",
    "\n",
    "i. 400 signals belongs to right hand class\n",
    "\n",
    "ii. 500 signals belongs to right foot class\n",
    "\n",
    "iii. Therefore, a total of 900 signals for right hand and right foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data and only select any one channel for simplicity\n",
    "Right_hand = filtered_signal_RH[:,102] # Select any one channel but not the channel should be same for RH and RF\n",
    "Right_foot = filtered_signal_RF[:,102] # Select any one channel but not the channel should be same for RH and RF\n",
    "\n",
    "Right_hand = Right_hand.reshape(400, 1000) # Reshape the right hand data of second channel into 10 second (100 Hz *10 sec)\n",
    "Right_foot = Right_foot.reshape(500, 1000) # Reshape the right foot data of second channel into 10 second (100 Hz *10 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca32c30",
   "metadata": {},
   "source": [
    "### Now create a binary labels indicating RH and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d31556",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((np.zeros((400,1)), np.ones((500,1))), axis=0) \n",
    "X = np.concatenate((Right_hand,Right_foot)) # 0 indicate RH and 1 indicate RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca880ddc",
   "metadata": {},
   "source": [
    "## This section indicate feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf53f5",
   "metadata": {},
   "source": [
    "#### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Features\n",
    "def extract_statistical_features(signal):\n",
    "    mean = np.mean(signal)\n",
    "    median = np.median(signal)\n",
    "    std_dev = np.std(signal)\n",
    "    variance = np.var(signal)\n",
    "    skewness = stats.skew(signal)\n",
    "    kurtosis = stats.kurtosis(signal)\n",
    "    range_val = np.ptp(signal)\n",
    "    \n",
    "    features = {\n",
    "        'Mean': mean,\n",
    "        'Median': median,\n",
    "        'Standard Deviation': std_dev,\n",
    "        'Variance': variance,\n",
    "        'Skewness': skewness,\n",
    "        'Kurtosis': kurtosis,\n",
    "        'Range': range_val\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "stat_features = [extract_statistical_features(signal) for signal in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec946d80",
   "metadata": {},
   "source": [
    "#### Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Domain Features\n",
    "def extract_time_domain_features(signal):\n",
    "    rms = np.sqrt(np.mean(signal**2))\n",
    "    zero_crossings = ((signal[:-1] * signal[1:]) < 0).sum()\n",
    "    autocorrelation = np.correlate(signal, signal, mode='full')[len(signal)-1]\n",
    "    mean_abs_dev = np.mean(np.abs(signal - np.mean(signal)))\n",
    "    max_val = np.max(signal)\n",
    "    min_val = np.min(signal)\n",
    "    signal_energy = np.sum(signal**2)\n",
    "    \n",
    "    features = {\n",
    "        'RMS': rms,\n",
    "        'Zero Crossings': zero_crossings,\n",
    "        'Autocorrelation': autocorrelation,\n",
    "        'Mean Absolute Deviation': mean_abs_dev,\n",
    "        'Max Value': max_val,\n",
    "        'Min Value': min_val,\n",
    "        'Signal Energy': signal_energy\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "time_features = [extract_time_domain_features(signal) for signal in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea77306",
   "metadata": {},
   "source": [
    "#### Frequency Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3acf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch  # Import the welch function\n",
    "# Frequency Domain Features\n",
    "def extract_frequency_domain_features(signal):\n",
    "    fs = 100\n",
    "    freqs, psd = welch(signal, fs)\n",
    "    dominant_freq = freqs[np.argmax(psd)]\n",
    "    total_power = np.sum(psd)\n",
    "    band_power = np.sum(psd[(freqs >= 0.5) & (freqs <= 40)])\n",
    "    mean_freq = np.mean(freqs)\n",
    "    median_freq = np.median(freqs)\n",
    "    peak_freq = freqs[np.argmax(psd)]\n",
    "    freq_variance = np.var(freqs)\n",
    "    \n",
    "    features = {\n",
    "        'Dominant Frequency': dominant_freq,\n",
    "        'Total Power': total_power,\n",
    "        'Band Power (0.5-40 Hz)': band_power,\n",
    "        'Mean Frequency': mean_freq,\n",
    "        'Median Frequency': median_freq,\n",
    "        'Peak Frequency': peak_freq,\n",
    "        'Frequency Variance': freq_variance\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "freq_features = [extract_frequency_domain_features(signal) for signal in X] # fs is a sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44163dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import antropy as ant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66772722",
   "metadata": {},
   "source": [
    "#### Entropy Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Features\n",
    "def extract_entropy_features(signal):\n",
    "    fs = 100\n",
    "    sample_entropy = ant.sample_entropy(signal)\n",
    "    spectral_entropy = ant.spectral_entropy(signal, sf=fs, method='welch')\n",
    "    perm_entropy = ant.perm_entropy(signal, normalize=True)\n",
    "    svd_entropy = ant.svd_entropy(signal, order=3, delay=1)\n",
    "    app_entropy = ant.app_entropy(signal)\n",
    "    lziv_complexity = ant.lziv_complexity(signal)\n",
    "    \n",
    "    features = {\n",
    "        'Sample Entropy': sample_entropy,\n",
    "        'Spectral Entropy': spectral_entropy,\n",
    "        'Permutation Entropy': perm_entropy,\n",
    "        'SVD Entropy': svd_entropy,\n",
    "        'Approximate Entropy': app_entropy,\n",
    "        'LZiv Complexity': lziv_complexity\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "entropy_features = [extract_entropy_features(signal) for signal in X] # fs is a sampling rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef1b76",
   "metadata": {},
   "source": [
    "### Feature concatenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into a single DataFrame\n",
    "def combine_features(stat_features, time_features, freq_features, entropy_features):\n",
    "    combined_features_list = []\n",
    "    for i in range(len(stat_features)):\n",
    "        combined_features = {**stat_features[i], **time_features[i], **freq_features[i], **entropy_features[i], }\n",
    "        combined_features_list.append(combined_features)\n",
    "    return pd.DataFrame(combined_features_list)\n",
    "\n",
    "combined_features_df = combine_features(stat_features, time_features, freq_features, entropy_features)\n",
    "#print(\"Combined Features DataFrame:\")\n",
    "#print(combined_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(combined_features_df)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler to normalize the data\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the result back into a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=combined_features_df.columns)\n",
    "print(normalized_df)\n",
    "print(normalized_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1c0db",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd75031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_fs(X,Y,n_features_to_select):\n",
    "    # Initialize the model\n",
    "    model = RandomForestClassifier()\n",
    "    \n",
    "    # Initialize RFE with the model and number of features to select\n",
    "    rfe = RFE(model, n_features_to_select=n_features_to_select)\n",
    "    \n",
    "    # Fit RFE\n",
    "    rfe = rfe.fit(X, Y)\n",
    "    \n",
    "    # Get the selected features\n",
    "    selected_features = X.columns[rfe.support_]\n",
    "    selected_features = selected_features.tolist()\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "#selected_features = RF_fs(normalized_df,Y,2)\n",
    "#print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f35995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# === GA for Feature Selection ===\n",
    "def genetic_algorithm(X, y, k=7, pop_size=10, generations=10, mutation_rate=0.2):\n",
    "    n_features = X.shape[1]\n",
    "    reg_par = 0.01  # Regularization parameter\n",
    "\n",
    "    def initialize_population():\n",
    "        return np.array([\n",
    "            np.random.permutation([1]*k + [0]*(n_features - k)) for _ in range(pop_size)\n",
    "        ])\n",
    "\n",
    "    def fitness(ind):\n",
    "        selected = np.where(ind == 1)[0]\n",
    "        if len(selected) == 0:\n",
    "            return 0\n",
    "        scores = cross_val_score(SVC(kernel='rbf'), X.iloc[:, selected], y, cv=5)\n",
    "        return scores.mean() - reg_par * (len(selected) / n_features)\n",
    "\n",
    "    def crossover(p1, p2):\n",
    "        merged = np.where((p1 + p2) > 0)[0]\n",
    "        selected = np.random.choice(merged, k, replace=False)\n",
    "        child = np.zeros(n_features, dtype=int)\n",
    "        child[selected] = 1\n",
    "        return child\n",
    "\n",
    "    def mutate(ind):\n",
    "        ones, zeros = np.where(ind == 1)[0], np.where(ind == 0)[0]\n",
    "        if len(ones) > 0 and len(zeros) > 0:\n",
    "            ind[np.random.choice(ones)] = 0\n",
    "            ind[np.random.choice(zeros)] = 1\n",
    "        return ind\n",
    "\n",
    "    def select_tournament(pop, scores):\n",
    "        participants = np.random.choice(len(pop), 3, replace=False)\n",
    "        return pop[participants[np.argmax([scores[i] for i in participants])]]\n",
    "\n",
    "    population = initialize_population()\n",
    "    for _ in range(generations):\n",
    "        scores = [fitness(ind) for ind in population]\n",
    "        new_pop = []\n",
    "        for _ in range(pop_size):\n",
    "            p1 = select_tournament(population, scores)\n",
    "            p2 = select_tournament(population, scores)\n",
    "            child = crossover(p1, p2)\n",
    "            if np.random.rand() < mutation_rate:\n",
    "                child = mutate(child)\n",
    "            new_pop.append(child)\n",
    "        population = np.array(new_pop)\n",
    "\n",
    "    final_scores = [fitness(ind) for ind in population]\n",
    "    best = population[np.argmax(final_scores)]\n",
    "    best = np.where(best == 1)[0]\n",
    "    selected_features = X.columns[best]\n",
    "    selected_features = selected_features.tolist()\n",
    "    return selected_features\n",
    "\n",
    "#selected_features = genetic_algorithm(normalized_df, Y, k=2, pop_size=10, generations=10, mutation_rate=0.2)\n",
    "\n",
    "#print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077257ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def cfs(X, y, max_features=7):\n",
    "    y = LabelEncoder().fit_transform(y)  # Encode class labels numerically\n",
    "\n",
    "    def cfs_merit(relevant_corr, redundant_corr, k):\n",
    "        if k == 0:\n",
    "            return 0\n",
    "        numerator = k * relevant_corr\n",
    "        denominator = np.sqrt(k + k * (k - 1) * redundant_corr)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def compute_correlations(X, y):\n",
    "        corr_with_target = []\n",
    "        for col in X.columns:\n",
    "            corr = np.abs(np.corrcoef(X[col], y)[0, 1])\n",
    "            corr_with_target.append(corr)\n",
    "        \n",
    "        corr_with_target = np.array(corr_with_target)\n",
    "        corr_matrix = np.abs(X.corr().values)  # Feature-feature correlations\n",
    "        return corr_with_target, corr_matrix\n",
    "\n",
    "\n",
    "    corr_target, corr_matrix = compute_correlations(X, y)\n",
    "    \n",
    "    selected = []\n",
    "    remaining = list(range(X.shape[1]))\n",
    "    best_merit = 0\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    for _ in range(max_features):\n",
    "        merits = []\n",
    "        for i in remaining:\n",
    "            test_subset = selected + [i]\n",
    "            k = len(test_subset)\n",
    "            rel = np.mean([corr_target[j] for j in test_subset])\n",
    "            # Compute redundancy only if there are at least 2 features\n",
    "            if len(test_subset) > 1:\n",
    "                redundancy_vals = [corr_matrix[j][k] for j in test_subset for k in test_subset if j != k]\n",
    "                red = np.nanmean(redundancy_vals) if redundancy_vals else 0\n",
    "            else:\n",
    "                red = 0  # No redundancy with only one feature\n",
    "            merit = cfs_merit(rel, red, k)\n",
    "            merits.append((i, merit))\n",
    "        \n",
    "        if merits:\n",
    "            i_best, merit_best = max(merits, key=lambda x: x[1])\n",
    "            selected.append(i_best)\n",
    "            remaining.remove(i_best)\n",
    "        else:\n",
    "            break  # No further improvement\n",
    "    \n",
    "    return [feature_names[i] for i in selected]\n",
    "\n",
    "#selected_features = cfs(normalized_df, Y,2)\n",
    "#print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e07295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyHSICLasso import HSICLasso\n",
    "\n",
    "def HSICLasso_fs(X, Y, n_features_to_select=2):\n",
    "    # Initialize HSIC Lasso\n",
    "    hsic_lasso = HSICLasso()\n",
    "\n",
    "    # Provide input data\n",
    "    X_lasso = X.to_numpy()\n",
    "    Y_lasso = Y.values.ravel() if isinstance(Y, pd.Series) else np.ravel(Y)\n",
    "\n",
    "    # Provide input data\n",
    "    hsic_lasso.input(X_lasso, Y_lasso, featname=X.columns.tolist())\n",
    "\n",
    "    # Perform classification to select top n_features_to_select features\n",
    "    hsic_lasso.classification(n_features_to_select)\n",
    "\n",
    "    # Retrieve names of selected features (if feature names were provided)\n",
    "    selected_features = hsic_lasso.get_features()\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "#selected_features = HSICLasso_fs(normalized_df, Y, n_features_to_select=2)\n",
    "#print(\"Selected feature names:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fdc5f9",
   "metadata": {},
   "source": [
    "### Classification using SVM (linear and nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVM and train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix', folder_name=None):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Right hand', 'Right foot'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.savefig(folder_name + title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3833ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, name, feature_names=None, folder_name=None):\n",
    "    # Convert X to NumPy array if it's a DataFrame\n",
    "    if hasattr(X, \"values\"):\n",
    "        X = X.values\n",
    "\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                         np.linspace(y_min, y_max, 500))\n",
    "\n",
    "    # Predict on grid points\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot contours\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')  # Decision boundary\n",
    "\n",
    "    # Scatter plot of data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.xlabel(feature_names[0] if feature_names else 'Feature 1')\n",
    "    plt.ylabel(feature_names[1] if feature_names else 'Feature 2')\n",
    "    plt.title(name)\n",
    "    plt.savefig(folder_name + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cc54f",
   "metadata": {},
   "source": [
    "### ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_test, y_pred, name='ROC Curve', folder_name=None):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(folder_name + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd5314",
   "metadata": {},
   "source": [
    "### Performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def performance_assessment(filtered_signal_RH,filtered_signal_RF,fs_method, n_features=2, channels=[0,1], folder_path=None):\n",
    "\n",
    "    def accuracy(confusion_matrix):\n",
    "        TN = confusion_matrix[0, 0]\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        FP = confusion_matrix[0, 1]\n",
    "        FN = confusion_matrix[1, 0]\n",
    "        return 100 * ((TP + TN) / (TP + TN + FP + FN)) if (TP + TN + FP + FN) != 0 else 0\n",
    "    \n",
    "    def precision(confusion_matrix):\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        FP = confusion_matrix[0, 1]\n",
    "        return 100 * (TP / (TP + FP)) if (TP + FP) != 0 else 0\n",
    "    \n",
    "    def specificiy(confusion_matrix):\n",
    "        TN = confusion_matrix[0, 0]\n",
    "        FP = confusion_matrix[0, 1]\n",
    "        return 100 * (TN / (TN + FP)) if (TN + FP) != 0 else 0\n",
    "    \n",
    "    def sensitivity(confusion_matrix):\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        FN = confusion_matrix[1, 0]\n",
    "        return 100 * (TP / (TP + FN)) if (TP + FN) != 0 else 0\n",
    "    \n",
    "    def F1_score(precision, sensitivity):\n",
    "        return (2 * (precision * sensitivity) / (precision + sensitivity)) if (precision + sensitivity) != 0 else 0\n",
    "    \n",
    "    # Define the grid of hyperparameters to search\n",
    "    param_grid = {\n",
    "        'C': [2, 10, 100, 1000],  # Regularization parameter\n",
    "        'gamma': [1, 0.5, 0.1],  # Kernel coefficient\n",
    "        'kernel': ['rbf']  # Kernel type\n",
    "    }\n",
    "    \n",
    "    for channel in range(channels[0]-1, channels[1]):\n",
    "\n",
    "        # Reshape the data and only select any one channel for simplicity\n",
    "        Right_hand = filtered_signal_RH[:,channel] # Select any one channel but not the channel should be same for RH and RF\n",
    "        Right_foot = filtered_signal_RF[:,channel] # Select any one channel but not the channel should be same for RH and RF\n",
    "\n",
    "        Right_hand = Right_hand.reshape(400, 1000) # Reshape the right hand data of second channel into 10 second (100 Hz *10 sec)\n",
    "        Right_foot = Right_foot.reshape(500, 1000) # Reshape the right foot data of second channel into 10 second (100 Hz *10 sec)\n",
    "\n",
    "        Y = np.concatenate((np.zeros((400,1)), np.ones((500,1))), axis=0) \n",
    "        X = np.concatenate((Right_hand,Right_foot)) # 0 indicate RH and 1 indicate RF\n",
    "\n",
    "        # Feature extraction\n",
    "        stat_features = [extract_statistical_features(signal) for signal in X]\n",
    "        time_features = [extract_time_domain_features(signal) for signal in X]\n",
    "        freq_features = [extract_frequency_domain_features(signal) for signal in X] # fs is a sampling rate\n",
    "        entropy_features = [extract_entropy_features(signal) for signal in X] # fs is a sampling rate\n",
    "\n",
    "        # Combine all features into a single DataFrame\n",
    "        combined_features_df = combine_features(stat_features, time_features, freq_features, entropy_features)\n",
    "        \n",
    "        # Create DataFrame and normalize the data\n",
    "        df = pd.DataFrame(combined_features_df)\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(df)\n",
    "        normalized_df = pd.DataFrame(normalized_data, columns=combined_features_df.columns)\n",
    "        \n",
    "        # Define 5-fold cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Initialize a matrix to store the combined confusion matrix\n",
    "        combined_confusion_matrix = np.zeros((len(np.unique(Y)), len(np.unique(Y))), dtype=int)\n",
    "\n",
    "        # Initialize lists to store all true labels and predicted probabilities\n",
    "        y_true_combined = []\n",
    "        y_pred_combined = []\n",
    "\n",
    "        # Perform cross-validation manually to access each fold\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(normalized_df), start=1):\n",
    "            X_train, X_test = normalized_df.iloc[train_index], normalized_df.iloc[test_index]\n",
    "            y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            selected_features = fs_method(X_train, y_train, n_features)\n",
    "            X_selected_train = X_train[selected_features]\n",
    "\n",
    "            # Define the SVM model\n",
    "            svm = SVC(probability=True)\n",
    "\n",
    "            # Perform grid search with cross-validation\n",
    "            grid_search = GridSearchCV(svm, param_grid, cv=10, scoring='balanced_accuracy')\n",
    "            grid_search.fit(X_selected_train, y_train)\n",
    "            best_svm = grid_search.best_estimator_\n",
    "            \n",
    "            X_selected_test = X_test[selected_features]\n",
    "            # Get probability predictions for ROC curve\n",
    "            y_pred = best_svm.predict(X_selected_test)\n",
    "            \n",
    "            # Append true labels and predicted probabilities to combined lists\n",
    "            y_true_combined.extend(y_test)\n",
    "            y_pred_combined.extend(y_pred)\n",
    "            \n",
    "            # Calculate confusion matrix for this fold\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Combine confusion matrix (sum them)\n",
    "            combined_confusion_matrix += cm\n",
    "\n",
    "            # Plot decision boundary\n",
    "            plot_decision_boundary(best_svm, X_selected_train, y_train, f'SVM Decision Boundary - Channel {channel + 1} - Fold {fold}', feature_names=selected_features, folder_name=folder_path)\n",
    "\n",
    "        # Plot confusion matrix for the combined results\n",
    "        plot_confusion_matrix(combined_confusion_matrix, title=f'Confusion Matrix - Channel {channel + 1}', folder_name=folder_path)\n",
    "        # Plot ROC curve\n",
    "        plot_roc_curve(y_true_combined, y_pred_combined, name=f'ROC Curve - Channel {channel + 1}', folder_name=folder_path)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        accuracy_score = accuracy(combined_confusion_matrix)\n",
    "        precision_score = precision(combined_confusion_matrix)\n",
    "        specificity_score = specificiy(combined_confusion_matrix)\n",
    "        sensitivity_score = sensitivity(combined_confusion_matrix)\n",
    "        f1_score = F1_score(precision_score, sensitivity_score)\n",
    "        print(f\"Performance Metrics for Channel {channel + 1}:\")\n",
    "        print(f\"Accuracy: {accuracy_score:.4f}\")\n",
    "        print(f\"Precision: {precision_score:.4f}\")\n",
    "        print(f\"Specificity: {specificity_score:.4f}\")\n",
    "        print(f\"Sensitivity: {sensitivity_score:.4f}\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Save data in file\n",
    "        with open(folder_path + f'performance_metrics_channel_{channel + 1}.txt', 'w') as f:\n",
    "            f.write(f\"Performance Metrics for Channel {channel + 1}:\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy_score:.4f}\\n\")\n",
    "            f.write(f\"Precision: {precision_score:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {specificity_score:.4f}\\n\")\n",
    "            f.write(f\"Sensitivity: {sensitivity_score:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {f1_score:.4f}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance_assessment(filtered_signal_RH,filtered_signal_RF,cfs, n_features=2, channels=[1,20], folder_path='cfs_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_assessment(filtered_signal_RH,filtered_signal_RF,RF_fs, n_features=2, channels=[1,20], folder_path='rf_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_assessment(filtered_signal_RH,filtered_signal_RF,HSICLasso_fs, n_features=2, channels=[1,20], folder_path='HSIC_Lasso_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba068779",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_assessment(filtered_signal_RH,filtered_signal_RF,genetic_algorithm, n_features=2, channels=[1,20], folder_path='GA_results/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
